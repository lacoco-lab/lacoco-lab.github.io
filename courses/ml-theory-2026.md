# Theory of Machine Learning for NLP

This lecture treats modern theory of machine learning (e.g., generalization bounds), with a special view to Language Models.
Content at a glance (tentative):
* Concentration Inequalities
* Generalization Bounds via Uniform Convergence
* Rademacher Complexity, Covering Numbers, VC Dimension
* Learning theory for LLMs/Transformers

Prerequisites:
* Understanding of machine learning
* Calculus and linear algebra
* Probability theory
* Strong motivation to work through mathematical content

To get an idea if your prerequisites are sufficient, you can check the early chapters in the notes linked under "Material" and see how accessible they look to you.

## Logistics

This lecture is offered in Summer Semester 2026.

When: TBD

Where: TBD

Lecturers: TBD

Credit Points: 6 CP

## Material

Our primary text will be [Tengyu Ma's lecture notes](https://github.com/tengyuma/cs229m_notes/blob/main/master.pdf) from [CS229M at Stanford](https://web.stanford.edu/class/stats214/) (see also [this iteration](https://tselilschramm.org/mltheory/stats214-fall24.html)). We will treat a selection of key topics from these notes.

We'll also take a view towards Transformers/Language Models, based on further readings (e.g., [Edelman et al 2022](https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf), [Wei et al 2021](https://arxiv.org/abs/2107.13163), [Hahn and Rofin 2024](https://arxiv.org/abs/2402.09963), [Huang et al 2025](https://openreview.net/forum?id=U49N5V51rU)).


## Syllabus

TBD

The syllabus will evolve over the course of the semester. We'll adjust selection of topics based on what works best.

## Grading

## Other
